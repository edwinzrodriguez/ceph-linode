# Copyright (C) 2020 Red Hat, Inc.
#
# This program is free software: you can redistribute it and/or modify it under
# the terms of the GNU General Public License as published by the Free Software
# Foundation, either version 3 of the License, or (at your option) any later
# version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT ANY
# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A
# PARTICULAR PURPOSE.  See the GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License along with
# this program.  If not, see <https://www.gnu.org/licenses/>.---


- name: preconfigure linodes
  import_playbook: pre-config.yml

- hosts: mons mgrs osds mdss grafana-servers clients
  become: yes
  tasks:
  - name: setup bootstrap monitor ssh key
    delegate_to: "{{ groups['mons'][0] }}"
    run_once: true
    community.crypto.openssh_keypair:
      path: "{{ ssh_user_home }}/.ssh/id_ed25519"
      type: ed25519
      comment: "{{ groups['mons'][0] }}"
    register: cephadm_key

  - name: register cephadm.id-ed25519 with hosts
    lineinfile:
      path: "{{ ssh_user_home }}/.ssh/authorized_keys"
      line: "{{ cephadm_key.public_key }}"

  - name: register cephadm.id-ed25519 with root user
    lineinfile:
      path: "/root/.ssh/authorized_keys"
      line: "{{ cephadm_key.public_key }}"

- hosts: mons
  become: yes
  tasks:
  - name: read cephadm private key from bootstrap monitor
    slurp:
      src: "{{ ssh_user_home }}/.ssh/id_ed25519"
    delegate_to: "{{ groups['mons'][0] }}"
    run_once: true
    register: cephadm_priv_key_b64

  - name: read cephadm public key from bootstrap monitor
    slurp:
      src: "{{ ssh_user_home }}/.ssh/id_ed25519.pub"
    delegate_to: "{{ groups['mons'][0] }}"
    run_once: true
    register: cephadm_pub_key_b64

  - name: distribute cephadm private key to mons
    copy:
      dest: "{{ ssh_user_home }}/.ssh/id_ed25519"
      content: "{{ cephadm_priv_key_b64.content | b64decode }}"
      owner: root
      group: root
      mode: '0600'
    when: inventory_hostname != groups['mons'][0]

  - name: distribute cephadm public key to mons
    copy:
      dest: "{{ ssh_user_home }}/.ssh/id_ed25519.pub"
      content: "{{ cephadm_pub_key_b64.content | b64decode }}"
      owner: root
      group: root
      mode: '0644'
    when: inventory_hostname != groups['mons'][0]

  - name: gather host keys by IP
    shell: "ssh-keyscan -T 5 -H -t rsa,ecdsa,ed25519 {{ hostvars[item].ansible_ssh_host | default(item) }} 2>/dev/null || true"
    loop: "{{ groups['mons'] + groups['mgrs'] + groups['osds'] + groups['mdss'] + groups['clients'] | unique }}"
    register: scan_ip
    changed_when: false
    failed_when: false

  - name: gather host keys by hostname
    shell: "ssh-keyscan -T 5 -H -t rsa,ecdsa,ed25519 {{ item }} 2>/dev/null || true"
    loop: "{{ groups['mons'] + groups['mgrs'] + groups['osds'] + groups['mdss'] + groups['clients'] | unique }}"
    register: scan_host
    changed_when: false
    failed_when: false

  - name: combine scanned host key lines
    set_fact:
      combined_known_host_lines: >-
        {{
          (
            ((scan_ip.results | map(attribute='stdout') | list | join('\n'))
             + '\n' +
             (scan_host.results | map(attribute='stdout') | list | join('\n')))
            .split('\n')
            | reject('match', '^#')
            | reject('equalto', '')
            | list
            | unique
          )
        }}

  - name: update known_hosts on mons
    known_hosts:
      name: "{{ item.split()[0] }}"
      key: "{{ item }}"
      path: "{{ ssh_user_home }}/.ssh/known_hosts"
      state: present
    loop: "{{ combined_known_host_lines }}"

- hosts: mons mgrs osds mdss clients
  become: yes
  tasks:
  - name: fetch cephadm
    become: no
    delegate_to: localhost
    run_once: true
    ansible.builtin.get_url:
      url: "https://download.ceph.com/rpm-{{ RELEASE }}/el9/noarch/cephadm"
      dest: "/tmp/cephadm-{{ RELEASE }}"
      mode: "0755"
      force: yes

  - name: distribute cephadm
    copy:
      dest: "{{ ssh_user_home }}/cephadm"
      group: root
      mode: 0755
      owner: root
      src: "/tmp/cephadm-{{ RELEASE }}"
      remote_src: false

  - name: "add {{ RELEASE }} repos"
    shell: "{{ ssh_user_home }}/cephadm add-repo {{ CEPHADM_REPO }}"

  - name: "install {{ RELEASE }}"
    shell: "{{ ssh_user_home }}/cephadm install ceph-common"

- hosts: mons
  become: yes
  tasks:
  - name: limit to first mon host
    when: inventory_hostname != groups['mons'][0]
    meta: end_host
  - name: reset cluster.spec (avoid mixed/invalid YAML from previous runs)
    file:
      path: "{{ ssh_user_home }}/cluster.spec"
      state: absent

  - name: read cluster.json (base)
    set_fact:
      cluster_cfg: "{{ lookup('file', 'cluster.json') | from_json }}"

  - name: locate osd node config (group=osds)
    set_fact:
      osd_node_cfg: >-
        {{
          (cluster_cfg.nodes | selectattr('group', 'defined') | selectattr('group', 'equalto', 'osds') | list + [none]) | first
          or
          (cluster_cfg.nodes | selectattr('groups', 'defined') | selectattr('groups', 'contains', 'osds') | list + [none]) | first
        }}

  - name: validate cluster.json has an osds node entry
    fail:
      msg: >-
        cluster.json is missing a nodes[] entry with group/groups containing "osds".
        Add an OSD node definition that includes "osd_devices_by_host".
    when: osd_node_cfg is none

  - name: read cluster.json (for per-host OSD devices)
    set_fact:
      osd_devices_by_host: "{{ (osd_node_cfg | default({}, true)).get('osd_devices_by_host', {}) }}"

  - name: set osd_hosts fact
    set_fact:
      osd_hosts: >-
        {%- set out = [] -%}
        {%- if osd_devices_by_host | length > 0 -%}
          {%- set out = osd_devices_by_host.keys() | list -%}
        {%- else -%}
          {%- for node in cluster_cfg.nodes -%}
            {%- set gs = node.get('groups', []) -%}
            {%- if (gs | length) == 0 and (node.get('group') is not none) -%}
              {%- set gs = [node.get('group')] -%}
            {%- endif -%}
            {%- if 'osds' in gs -%}
              {%- for i in range((node.get('count') | int)) -%}
                {%- set _ = out.append('%s-%03d' % (node.get('prefix'), i)) -%}
              {%- endfor -%}
            {%- endif -%}
          {%- endfor -%}
        {%- endif -%}
        {{ out }}

  - name: validate every OSD host has a device list in cluster.json
    fail:
      msg: >-
        Missing OSD device list for host '{{ item }}'.
        Add it under nodes[].osd_devices_by_host in cluster.json, e.g.:
        "osd_devices_by_host": { "{{ item }}": ["/dev/sdX"] }
    loop: "{{ osd_hosts }}"
    when: (osd_devices_by_host.get(item, []) | length) == 0

  - name: write cephadm spec (single clean YAML stream)
    copy:
      dest: "{{ ssh_user_home }}/cluster.spec"
      owner: root
      group: root
      mode: "0644"
      content: |
        {# Build host_list and labels_by_host from inventory groups #}
        {% set host_list = [] %}
        {% set labels_by_host = {} %}

        {% for g in ['mons', 'osds', 'mdss', 'mgrs', 'clients'] %}
        {%   for h in groups.get(g, []) %}
        {%     if h not in host_list %}
        {%       set _ = host_list.append(h) %}
        {%     endif %}
        {%     if h not in labels_by_host %}
        {%       set _ = labels_by_host.update({h: []}) %}
        {%     endif %}
        {%     if g not in labels_by_host[h] %}
        {%       set _ = labels_by_host[h].append(g) %}
        {%     endif %}
        {%   endfor %}
        {% endfor %}

        {# Host specs #}
        {% for h in host_list %}
        {% if not loop.first %}---
        {% endif %}
        service_type: host
        hostname: {{ h }}
        addr: {{ hostvars[h]['ansible_ssh_host'] }}
        labels:
        {% for lab in labels_by_host.get(h, []) %}
          - {{ lab }}
        {% endfor %}
        {% if 'mons' in labels_by_host.get(h, []) %}
          - _admin
        {% endif %}
        {% endfor %}

        ---
        service_type: mon
        placement:
          label: mons

        ---
        service_type: mgr
        placement:
          label: mgrs

        {# Per-host OSD service specs #}
        {% for h in osd_hosts %}
        ---
        service_id: {{ h }}
        service_type: osd
        placement:
          hosts:
            - {{ h }}
        data_devices:
          paths:
        {% for dev in osd_devices_by_host.get(h, []) %}
            - {{ dev }}
        {% endfor %}
        {% endfor %}


  - name: check ceph already bootstrapped
    stat:
      path: /etc/ceph/ceph.conf
    register: ceph_installed

  - name: "cephadm bootstrap"
    shell: "{{ ssh_user_home }}/cephadm --image {{ CEPH_IMAGE }} bootstrap --mon-ip {{ monitor_address }} --ssh-private-key {{ ssh_user_home }}/.ssh/id_ed25519 --ssh-public-key {{ ssh_user_home }}/.ssh/id_ed25519.pub --apply-spec {{ ssh_user_home }}/cluster.spec --skip-mon-network"
    when: not ceph_installed.stat.exists

  - name: config public network
    shell: "ceph config set global public_network {{ hostvars[groups['mons'][0]]['public_network'] }}"
    when:
      - hostvars[groups['mons'][0]]['public_network'] is defined
      - hostvars[groups['mons'][0]]['public_network'] != "-"
      - hostvars[groups['mons'][0]]['public_network'] != hostvars[groups['mons'][0]]['cluster_network']
      - hostvars[groups['mons'][0]]['public_network'].split('.')[0:3] | join('.') in ansible_all_ipv4_addresses | map('regex_replace', '^(.*)\..*$', '\\1') | list

  - name: config cluster_network
    shell: "ceph config set global cluster_network {{ hostvars[groups['mons'][0]]['cluster_network'] }}"
    when:
      - hostvars[groups['mons'][0]]['cluster_network'] is defined
      - hostvars[groups['mons'][0]]['cluster_network'] != "-"
      - hostvars[groups['mons'][0]]['cluster_network'].split('.')[0:3] | join('.') in ansible_all_ipv4_addresses | map('regex_replace', '^(.*)\..*$', '\\1') | list

  - name: log to files
    shell: ceph config set global log_to_file true

  - name: log cluster to file
    shell: ceph config set global mon_cluster_log_to_file true

  - name: disable stderr logging
    shell: ceph config set global log_to_stderr false

  - name: disable stderr logging of cluster log
    shell: ceph config set global mon_cluster_log_to_stderr false

- hosts: clients
  become: yes
  tasks:
  - name: "add {{ RELEASE }} repos"
    shell: "{{ ssh_user_home }}/cephadm add-repo {{ CEPHADM_REPO }}"

  - name: "install {{ RELEASE }}"
    shell: "{{ ssh_user_home }}/cephadm install ceph-common ceph-fuse"

  - name: generate min config
    shell: ceph config generate-minimal-conf
    delegate_to: "{{ groups['mons'][0] }}"
    register: minimal_config
    run_once: true

  - name: setup config
    copy:
      # yay ansible bug: https://github.com/ansible/ansible/issues/6077
      content: "{{ minimal_config.stdout }}\n"
      dest: /etc/ceph/ceph.conf
      owner: root
      group: root
      mode: 0644
