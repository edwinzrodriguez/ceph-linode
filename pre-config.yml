# Copyright (C) 2020 Red Hat, Inc.
#
# This program is free software: you can redistribute it and/or modify it under
# the terms of the GNU General Public License as published by the Free Software
# Foundation, either version 3 of the License, or (at your option) any later
# version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT ANY
# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A
# PARTICULAR PURPOSE.  See the GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License along with
# this program.  If not, see <https://www.gnu.org/licenses/>.


# This playbook does some basic pre-configuration of the Linodes prior to Ceph installation. In particular, it:
#
# - updates packages
# - sets up log rotation
# - configures the core dump location for easy retrieval via other ansible playbooks
# - tries to wipe the /dev/sdc device on OSDs (only works if the OSD is down);
#   this is necessary after nuking the cluster as Linode does not wipe the disks
#   immediately after deletion.

---

- hosts: all
  become: yes
  tasks:
   - name: set hostname
     ansible.builtin.hostname:
       name: "{{ inventory_hostname }}"

   - name: create hosts from inventory
     become: no
     shell: ./misc/generate-hosts.py linodes
     register: etchosts
     run_once: true
     delegate_to: localhost

   - name: add to /etc/hosts
     blockinfile:
       block: "{{ etchosts.stdout }}"
       path: /etc/hosts

   - name: add to /etc/hosts
     blockinfile:
       block: "{{ etchosts.stdout }}"
       path: /etc/hosts
     ignore_errors: yes
     run_once: true
     delegate_to: localhost

   # coredump capture setup
   - name: create crash directory
     file:
       path: /crash
       state: directory
       owner: root
       group: root
       mode: 1777

   - name: set core_pattern
     sysctl:
       name: kernel.core_pattern
       value: /crash/%e-%h-%p-%t.core

   - name: add DefaultLimitCORE=infinity
     lineinfile:
       path: /etc/systemd/system.conf
       line: DefaultLimitCORE=infinity
     register: coreline

   - name: reexec systemd
     shell: systemctl daemon-reexec
     when: coreline.changed

   - name: import EPEL GPG key
     ansible.builtin.rpm_key:
       state: present
       key: https://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-9

   - name: import EPEL-NEXT GPG key (best-effort)
     ansible.builtin.rpm_key:
       state: present
       key: https://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-9
     ignore_errors: yes

   - name: install epel-release RPMs
     yum:
       name:
         - "https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm"
         - "https://dl.fedoraproject.org/pub/epel/epel-next-release-latest-9.noarch.rpm"
       state: present
       update_cache: yes

   - name: enable epel repo
     yum_repository:
       name: epel
       description: "Extra Packages for Enterprise Linux 9 - $basearch"
       metalink: "https://mirrors.fedoraproject.org/metalink?repo=epel-9&arch=$basearch"
       enabled: yes
       gpgcheck: yes
       gpgkey: "https://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-9"
       state: present

   - name: enable epel-next repo (if available)
     yum_repository:
       name: epel-next
       description: "EPEL Next for Enterprise Linux 9 - $basearch"
       metalink: "https://mirrors.fedoraproject.org/metalink?repo=epel-next-9&arch=$basearch"
       enabled: yes
       gpgcheck: yes
       gpgkey: "https://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-9"
       state: present
     ignore_errors: yes

   - name: update packages
     yum:
       name: "*"
       state: latest
       update_cache: yes

  # package setup
  # - logrotate needs psmisc for killall
  # - ceph-ansible needs EPEL but also needs ca-certificates installed to avoid some errors, do this now
   - name: install launcher packages
     yum:
       name: logrotate,psmisc,ca-certificates,python3,chrony,podman,lvm2
       state: latest

   # log rotation setup
   - name: copy ceph-logrotate.timer
     copy: src=ceph-log-rotate.timer dest=/etc/systemd/system/ owner=root group=root mode=644

   - name: copy ceph-logrotate.service
     copy: src=ceph-log-rotate.service dest=/etc/systemd/system/ owner=root group=root mode=644

   - name: copy ceph logrotate config
     copy: src=ceph.logrotate dest=/etc/logrotate.d/ceph owner=root group=root mode=644

 # Ideally we would use the ceph-logrotate.timer but systemd has a bug requiring cron
   - name: copy crontab
     copy: src=crontab dest=/root/ owner=root group=root mode=644
   - name: register crontab
     shell: crontab crontab

   - name: enable ntp
     systemd:
       enabled: yes
       name: chronyd
       state: started

- hosts: mdss
  become: yes
  tasks:
   - name: check for mds logrotate config
     local_action: stat path="ceph-mds.logrotate"
     register: ceph_mds_logrotate
     become: false

   - name: copy ceph mds logrotate config
     copy: src=ceph-mds.logrotate dest=/etc/logrotate.d/ceph owner=root group=root mode=644
     when: ceph_mds_logrotate.stat.exists

- hosts: osds
  become: yes
  tasks:
  # This cleans up after a nuke operation (disks are not wiped)
  # This will fail if the lvm volume is busy
  - name: clear old lvm volumes
    shell: lvremove --yes /dev/ceph*/*
    ignore_errors: yes

  - name: read cluster.json (base)
    set_fact:
      cluster_cfg: "{{ lookup('file', 'cluster.json') | from_json }}"

  - name: locate osd node config (group=osds)
    set_fact:
      osd_node_cfg: >-
        {{
          (cluster_cfg.nodes | selectattr('group', 'defined') | selectattr('group', 'equalto', 'osds') | list + [none]) | first
          or
          (cluster_cfg.nodes | selectattr('groups', 'defined') | selectattr('groups', 'contains', 'osds') | list + [none]) | first
        }}

  - name: validate cluster.json has an osds node entry
    fail:
      msg: >-
        cluster.json is missing a nodes[] entry with group/groups containing "osds".
        Add an OSD node definition that includes "osd_devices_by_host".
    when: osd_node_cfg is none

  - name: compute osd_devices_by_host from cluster.json
    set_fact:
      osd_devices_by_host: "{{ osd_node_cfg.get('osd_devices_by_host', {}) }}"

  - name: compute devices to wipe for this host
    set_fact:
      osd_devices_to_wipe: "{{ osd_devices_by_host.get(inventory_hostname, []) }}"

  - name: validate this OSD host has a device list in cluster.json
    fail:
      msg: >-
        Missing OSD device list for host '{{ inventory_hostname }}'.
        Add it under nodes[].osd_devices_by_host in cluster.json, e.g.:
        "osd_devices_by_host": { "{{ inventory_hostname }}": ["/dev/sdX"] }
    when: (osd_devices_to_wipe | length) == 0
  # This cleans up after a nuke operation (disks are not wiped)
  # This will fail if the lvm volume is busy
  - name: wipe OSD devices
    shell: "wipefs -a {{ item }}"
    loop: "{{ osd_devices_to_wipe }}"
    ignore_errors: yes

  - name: check for osd logrotate config
    local_action: stat path="ceph-osd.logrotate"
    register: ceph_osd_logrotate
    become: false

  - name: copy ceph osd logrotate config
    copy: src=ceph-osd.logrotate dest=/etc/logrotate.d/ceph owner=root group=root mode=644
    when: ceph_osd_logrotate.stat.exists

- hosts: mons
  become: yes
  tasks:
   - name: check for mon logrotate config
     local_action: stat path="ceph-mon.logrotate"
     register: ceph_mon_logrotate
     become: false

   - name: copy ceph mon logrotate config
     copy: src=ceph-mon.logrotate dest=/etc/logrotate.d/ceph owner=root group=root mode=644
     when: ceph_mon_logrotate.stat.exists

- hosts: clients
  become: yes
  tasks:
  - name: install nfs-utils
    yum: name=nfs-utils state=latest update_cache=yes

#- import_playbook: iptables.yml
